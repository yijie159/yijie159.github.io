---
title: 深度学习
date: 2025-10-13 16:03:22
tags:
- 深度学习
- Python
---

# 深度学习

## 深度学习的特点

- 使用多层神经网络，能够自动提取数据的多层次特征
- 适合处理非结构化数据，如图像、音频、文本等
- 依赖大量数据和计算资源，训练时间较长
- 模型复杂，通常被视为“黑箱”，解释性较差 
<!--more-->
## 1、Matplotlib

### 绘制简单图形

#### 代码

```Python
import numpy as np
import matplotlib.pyplot as plt

if __name__ == '__main__':

    # 生成数据
    x = np.arange(0,6,0.1) #以0.1为单位，生成0到6得数据
    y1 = np.sin(x)
    y2 = np.cos(x)

    plt.plot(x,y1,label="sin(x)")
    plt.plot(x,y2,linestyle = "--",label = "cos(x)") #用虚线绘制
    plt.xlabel("x")# x轴标签
    plt.ylabel("y")# y轴标签
    plt.title('sin & cos') #标题
    plt.legend()
    plt.show()
```

#### 图像生成

### 显示图像

#### 描述

pyplot中提供了用于显示图像得方法imshow()。可以使用matplotlib.image模块得imread()方法读入图像

#### 代码

```Python
# 显示图像
def showImage():
    img = imread('D:\\PycharmProjects\\PythonProject\\image\\zwz.jpg')
    plt.imshow(img)

    plt.show()
```

## 2、感知机

### 2.1、什么是感知机

感知机接受多个输入信号，输出一个信号，通过蒸馏提炼的方式。信号可以理解为像河流、电流那样，具备“流动性”的东西，像前面输送东西，但是感知机的信号只有“流/不流”(1/0)两种取值，以下所有的学习当中，0对应“不传递信息”，1对应“传递信号”。

输入信号被送往神经元时，会被分别乘以固定的权重。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也就称为“神经元被激活”。这里将这个界限值称为阈值。

感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。即权重越大，对应该权重的信号的重要性就越高

### 2.2、简单逻辑电路

#### 2.2.1 与门

与门是有两个输入和一个输出的门电路，与门仅在两个输入均为1时输出1，其他时候则输出0。

##### 公式

$$
y=\left\{\begin{array}{l} 0\ (b+w_{1}x_{1}+w_{2}x_{2}\leq 0)\\ 1\ (b+w_{1}x_{1}+w_{2}x_{2}>0)\end{array}\right.
$$

##### 代码

```python
def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
if __name__ == '__main__':
    # 与门
    print(AND(0,0)) # 输出0
    print(AND(1,0)) # 输出0
    print(AND(0,1)) # 输出0
    print(AND(1,1)) # 输出1
```

#### 2.2.2、与非门和或门

与非门(NAND gate)。NAND是Not AND的意思，颠倒了与门的输出。即当两个输出单元同时为1时输出0，其他时候输出1

##### 代码

```python
#感知机的简单实现
def NAND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([-0.5,-0.5])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

if __name__ == '__main__':
    # 与非门
    print(NAND(0,0)) # 输出1
    print(NAND(1,0)) # 输出1
    print(NAND(0,1)) # 输出1
    print(NAND(1,1)) # 输出0
```

与非门的实现如上

或门：

```Py
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5, 0.5])
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

#### 2.3.2、导入权重和偏置

​	感知机会计算输入信号和权重的乘积，然后加上偏置，如果这个值大于0则输出1，反之输出0

#### 2.3.3、使用权重和偏置的实现

##### 代码

```Python
# 使用权重和偏置的实现
import numpy as np
import matplotlib.pyplot as plt
def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

if __name__ == '__main__':
    print(AND(0, 0))
    print(AND(1, 0))
    print(AND(0, 1))
    print(AND(1, 1))  
```

​	偏置和权重w1，w2的作用是不一样的。具体的说，w1和w2是控制输入信号的重要性的参数，而<span style="color:red;">偏置是调整个神经元被激活的容易程度</span>（输出信号为1的程度）的参数。比如，<span style = "color:red">若b为-0.1，则只要输入信号的加权总和超过0.1</span>，神经元就会被激活.但是如果b为-20.0，则需要输入信号的加权总和必须超过20.0，这样神经元才会被激活。因此偏置的值决定了神经元被激活的容易程度。

### 2.4、感知机的局限性

#### 2.4.1、异或门

​	异或门也称为逻辑异或或电路(仅当两个输入信号中的一方为1时，才会输出1)。然而以上的介绍的感知机并不能实现异或门，无论用与门，与非门亦或是或门都只会生成一条直线，分割两个区域。

#### 2.4.2、线性和非线性

​	根据上个观点所述，如果使用上述感知机无法实现异或门，但是将“直线”这个限制条件去掉就可以实现。

​	感知机的局限性就是在于它<span style = "color:red">只能表示用一条直线分割的空间，像异或门那样需要弯曲的曲线无法用感知机表示</span>，另外曲线分割而成的空间称为非线性空间，由直线分割而成的空间称为线性空间。

![](深度学习/异或门.png)

### 2.5、多层感知机

​	虽然异或门并不能通过感知机实现，但是我们可以通过感知机进行<span style = "color:red">"叠加层"</span>。

​	这里使用的方法就是<span style="color:red">组合与门、与非门、或门进行配置</span>

#### 2.5.1、已有门电路的组合

​	对与门、与非门、或门进行配置![异或多层感知机思路整理](深度学习/异或多层感知机思路整理.png)

通过组合与门、与非门、或门实现异或门:

![](深度学习/与非门.png)

与或门的真值表：

![](深度学习/与或门真值表.png)



## **激活函数**

​	激活函数是连接感知机和神经网络得桥梁，在神经网络中起着至关重要得作用。

​	如果没有激活函数，整个神经网络就等效于单层线性变换，不论如何加深层数，总是存在与之等效得“无隐藏层得神经网络”。激活函数必须是非线性函数，也正是激活函数得存在为神经网络引入了非线性，使得神经网络能够学习和表示复杂得非线性关系

### 阶跃函数（Binary step）

​	感知机中，表示的h(x)就是最简单的激活函数，它可以为输入设置一个阈值；一旦超过这个阈值，就切换输出（0或者1）.这种函数被称为“阶跃函数”。!

### Sigmoid函数

    待补充

#### 代码

```Python
# Sigmoid函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

### Tanh函数

#### 概念

​	Tanh（双曲正切）将输入映射到区间(-1,1)。其关于原点中心对称。常用在隐藏层。

​	输入再[-3,3]之外时，Tanh的输出值变化很小，此时其导数接近0。

​	Tanh的输出以0为中心，且其**梯度相较于Sigmoid更大，收敛速度相对更快**。但同样**也存在梯度消失现象**

#### 代码

```Python
print(np.tanh(x))
```

### ReLu函数

#### 描述

​	ReLU(Rectified Linear Unit,修正线性单元)会将小于0的输入转换成0，大于等于0的输入则保持不变。ReLU定义简单，计算量小。常用于隐藏层。

​	ReLU作为激活函数**不存在梯度消失**。当**输入小于0时，ReLU的输出为0**，这意味着在神经网络中，ReLU激活的节点只有部分是“活跃”的，这种稀疏性有助于减少计算量和提高模型的效率。

#### 代码

```python
#ReLU函数
def relu(x):
    return np.maximum(0, x)
```

