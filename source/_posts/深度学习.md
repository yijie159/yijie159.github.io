---
title: 深度学习
date: 2025-10-13 16:03:22
tags:
    - 深度学习
    - Python
---

# 深度学习

## 深度学习的特点

- 使用多层神经网络，能够自动提取数据的多层次特征
- 适合处理非结构化数据，如图像、音频、文本等
- 依赖大量数据和计算资源，训练时间较长
- 模型复杂，通常被视为“黑箱”，解释性较差 
<!--more-->
## 1、Matplotlib

### 绘制简单图形

#### 代码

```Python
import numpy as np
import matplotlib.pyplot as plt

if __name__ == '__main__':

    # 生成数据
    x = np.arange(0,6,0.1) #以0.1为单位，生成0到6得数据
    y1 = np.sin(x)
    y2 = np.cos(x)

    plt.plot(x,y1,label="sin(x)")
    plt.plot(x,y2,linestyle = "--",label = "cos(x)") #用虚线绘制
    plt.xlabel("x")# x轴标签
    plt.ylabel("y")# y轴标签
    plt.title('sin & cos') #标题
    plt.legend()
    plt.show()
```

#### 图像生成

### 显示图像

#### 描述

​	pyplot中提供了用于显示图像得方法imshow()。可以使用matplotlib.image模块得imread()方法读入图像

#### 代码

```Python
# 显示图像
def showImage():
    img = imread('D:\\PycharmProjects\\PythonProject\\image\\zwz.jpg')
    plt.imshow(img)

    plt.show()
```

## 2、感知机

### 2.1、什么是感知机

​	感知机接受多个输入信号，输出一个信号，通过蒸馏提炼的方式。信号可以理解为像河流、电流那样，具备“流动性”的东西，像前面输送东西，但是感知机的信号只有“流/不流”(1/0)两种取值，以下所有的学习当中，0对应“不传递信息”，1对应“传递信号”。

​	输入信号被送往神经元时，会被分别乘以固定的权重。神经元会计算传送过来的信号的总和，**只有当这个总和超过了某个界限值时，**才会输出1。这也就称为“神经元被激活”。这里将这个界限值称为阈值。

​	感知机的多个输入信号**都有各自固有的权重**，这些权重发挥着控制各个信号的重要性的作用。**即权重越大，对应该权重的信号的重要性就越高**

### 2.2、简单逻辑电路

#### 2.2.1 与门

​	与门是有两个输入和一个输出的门电路，与门仅在两个输入均为1时输出1，其他时候则输出0。

#### 2.2.2、与非门和或门

​	与非门(NAND gate)。NAND是Not AND的意思，颠倒了与门的输出。即当两个输出单元同时为1时输出0，其他时候输出1

##### 代码

```python
#感知机的简单实现
def AND(x1,x2):
    w1,w2 ,theta = 0.5,0.5,0.7
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1

if __name__ == '__main__':
    # 与门
    print(AND(0,0)) # 输出0
    print(AND(1,0)) # 输出0
    print(AND(0,1)) # 输出0
    print(AND(1,1)) # 输出1
```

​	与门的实现如上，而与非门则可以通过取非完成。

2.3.2、导入权重和偏置

## **激活函数**

​	激活函数是连接感知机和神经网络得桥梁，在神经网络中起着至关重要得作用。

​	如果没有激活函数，整个神经网络就等效于单层线性变换，不论如何加深层数，总是存在与之等效得“无隐藏层得神经网络”。激活函数必须是非线性函数，也正是激活函数得存在为神经网络引入了非线性，使得神经网络能够学习和表示复杂得非线性关系

### 阶跃函数（Binary step）

​	感知机中，表示的h(x)就是最简单的激活函数，它可以为输入设置一个阈值；一旦超过这个阈值，就切换输出（0或者1）.这种函数被称为“阶跃函数”。!

### Sigmoid函数

    待补充

#### 代码

```Python
# Sigmoid函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

### Tanh函数

#### 概念

​	Tanh（双曲正切）将输入映射到区间(-1,1)。其关于原点中心对称。常用在隐藏层。

​	输入再[-3,3]之外时，Tanh的输出值变化很小，此时其导数接近0。

​	Tanh的输出以0为中心，且其**梯度相较于Sigmoid更大，收敛速度相对更快**。但同样**也存在梯度消失现象**

#### 代码

```Python
print(np.tanh(x))
```

### ReLu函数

#### 描述

​	ReLU(Rectified Linear Unit,修正线性单元)会将小于0的输入转换成0，大于等于0的输入则保持不变。ReLU定义简单，计算量小。常用于隐藏层。

​	ReLU作为激活函数**不存在梯度消失**。当**输入小于0时，ReLU的输出为0**，这意味着在神经网络中，ReLU激活的节点只有部分是“活跃”的，这种稀疏性有助于减少计算量和提高模型的效率。

#### 代码

```python
#ReLU函数
def relu(x):
    return np.maximum(0, x)
```

